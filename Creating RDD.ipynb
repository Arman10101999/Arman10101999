{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPhS7J8q6QigjxFgLO1k1aM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arman10101999/Arman10101999/blob/main/Creating%20RDD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHa3j5RpTlE9",
        "outputId": "dd4e979a-9781-47a0-8d19-795faa8d00c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 33 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 47.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=4321d8bef43b9f80ca32e870cdf22c14d57ddb3d7ead5f27fd72c40746a9bdd7\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"Python Spark create RDD example\") \\\n",
        ".config(\"spark.some.config.option\", \"some-value\") \\\n",
        ".getOrCreate()\n",
        "df = spark.sparkContext\\\n",
        ".parallelize([(1, 2, 3, 'a b c'),\n",
        "(4, 5, 6, 'd e f'),\n",
        "(7, 8, 9, 'g h i')])\\\n",
        ".toDF(['col1', 'col2', 'col3','col4'])\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvwl-V_bTxf8",
        "outputId": "07265237-43c8-4ed4-8e80-3866394e375d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+-----+\n",
            "|col1|col2|col3| col4|\n",
            "+----+----+----+-----+\n",
            "|   1|   2|   3|a b c|\n",
            "|   4|   5|   6|d e f|\n",
            "|   7|   8|   9|g h i|\n",
            "+----+----+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"Python Spark create RDD \") \\\n",
        ".config(\"spark.some.config.option\",\"some-value\") \\\n",
        ".getOrCreate()\n",
        "\n",
        "df = spark.sparkContext\\\n",
        ".parallelize([(1,2,3 ,'a b c'),(4 , 5 ,6 ,'d e f '),(7 , 8 , 9,'g h i')])\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "RjZJhUrGVLFI",
        "outputId": "33498bc2-13be-4db3-b230-6abf00f88333"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-e09a1921be21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m'a b c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m'd e f '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'g h i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'RDD' object has no attribute 'show'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark= SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"python spark create rdd using dataframe\") \\\n",
        ".config(\"spark,some.config.option\",\"some-value\") \\\n",
        ".getOrCreate()\n",
        "\n",
        "df= spark.createDataFrame([\n",
        "('1', 'Joe', '70000', '1'),\n",
        "('2', 'Henry', '80000', '2'),\n",
        "('3', 'Sam', '60000', '2'),\n",
        "('4', 'Max', '90000', '1')],\n",
        "['Id', 'Name', 'Sallary','DepartmentId']\n",
        ")\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zVyz8spW4ZX",
        "outputId": "dd042bb5-8e48-4d6f-d2eb-2e04abfb1af3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+-------+------------+\n",
            "| Id| Name|Sallary|DepartmentId|\n",
            "+---+-----+-------+------------+\n",
            "|  1|  Joe|  70000|           1|\n",
            "|  2|Henry|  80000|           2|\n",
            "|  3|  Sam|  60000|           2|\n",
            "|  4|  Max|  90000|           1|\n",
            "+---+-----+-------+------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}